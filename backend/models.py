from sentence_transformers import SentenceTransformer
from simpletransformers.classification import ClassificationModel

from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np
import re
from scipy.special import softmax

import logging


class Model:
    _use_cuda = False
    _DEVICE = 'cpu'

    def __init__(self, name, path_to_weights, cuda=False) -> None:
        if cuda:
            self._use_cuda = True
            self._DEVICE = 'cuda'

        self.model_for_embeddings = SentenceTransformer(name, device=self._DEVICE)
        self.classifier = ClassificationModel('bert', path_to_weights, use_cuda=self._use_cuda)

        logging.warning(f'\n Model device: {self._DEVICE}')

    def process_data(self, data):
        """
        data: list of sentences

        Compute embeddings and write to self filed - embeddings\n
        Compute cosine similarity btw all <count> sentences and write it to self field - cosine_matrix
        
        return pd.DataFrame with columns: ['text', 'category']
        """

        logging.warning(f'\n Model for embeddings: <MiniLM-L12-v2> \n Model for classification: <trained BERT>')

        data = pd.DataFrame({
            'text': data,
            'category': np.zeros(len(data)),
            'unique': np.ones(len(data))
        })

        data['text'] = data['text'].astype('str').apply(self.clean_text)
        logging.warning('\n Model: text cleaned')

        categorized_data_frame = self.make_classification(data)

        detected_data = self.detect_duplicate(categorized_data_frame)

        proc_data = self.delete_duplicate(detected_data)

        proc_data['category'] = proc_data['category'].apply(self.id2cat)

        proc_data.drop(['unique'], axis=1, inplace=True)

        logging.warning('\n Model: index turned to name')
        logging.warning('\n Model: Its all done')

        return proc_data

    def make_classification(self, data):
        
        logging.warning('\n Classification...')
        predict_classes, raw_outputs = self.classifier.predict(data['text'].values.tolist())

        proba = softmax(raw_outputs, axis=1)

        max_proba = np.partition(proba, -1)[:, -1]
        predict_classes[max_proba < 0.4] = -1

        mask_1 = data['text'].str.contains('–∫—É–ø–∏–ª')
        mask_2 = data['text'].str.contains('–ø—Ä–æ–¥–∞–ª')
        mask = mask_1 + mask_2
        mask = ~mask
        
        # data['proba'] = max_proba
        data['category'] = predict_classes*mask
        
        
        logging.warning(f'\n len predicted: {len(predict_classes)}, unique: {np.unique(predict_classes)}')
        logging.warning('\n Model: categorized done')

        return data

    def detect_duplicate(self, data, t=0.78):
        # –∏–Ω–¥–µ–∫—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–¥–∞–ª–∏–º
        all_indexes = []

        for category in data['category'].unique():
            temp_data = data[data['category'] == category].reset_index()
            list_of_sentences = temp_data['text'].tolist()

            # –ø–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤
            embeddings = self.model_for_embeddings.encode(list_of_sentences)

            # —Å—á–∏—Ç–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–∞–º–∏
            cosine = cosine_similarity(embeddings)

            # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
            cosine = np.where(cosine < t, False, True)

            # –∏–Ω–¥–µ–∫—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ç–æ–º —É–¥–∞–ª–∏–º
            indexes_to_drop = list()

            # –≤—ã–±–∏—Ä–∞–µ–º —ç—Ç–∏ –∏–Ω–¥–µ–∫—Å—ã
            for i in range(len(cosine)):
                temp_indexes = np.transpose((cosine[i][i + 1:]).nonzero())
                indexes_to_drop.extend(list(temp_indexes + 1))

            # –ø–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞, –∞ –Ω–µ –Ω–∞—à–µ–≥–æ —Å—Ä–µ–∑–∞ (–≥–¥–µ —Ç–æ–ª—å–∫–æ 1 –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π)
            true_indexes = (np.where(np.isin(temp_data.index.values, indexes_to_drop), temp_data['index'], -1))

            # —É–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            true_indexes = set(true_indexes)

            # —É–¥–∞–ª–∏–º -1 - (–∏–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥—É–±–ª–∏–∫–∞—Ç–æ–º)
            true_indexes.remove(-1)

            # –¥–æ–±–∞–≤–∏–º –≤–æ –≤—Å–µ –∏—Ç–æ–≥–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã
            all_indexes.extend(true_indexes)

        # –Ω–µ –±—É–¥–µ–º —É–¥–∞–ª—è—Ç—å –∑–∞—â–∏—â–µ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
        predicted_indexes = set(self.get_trades(data))
        indexes_to_drop = list(set(all_indexes) - set(predicted_indexes))

        # –¥—Ä–æ–ø–∞–µ–º
        data.drop(indexes_to_drop, inplace=True)
        data.reset_index(inplace=True, drop=True)

        return data

    def get_trades(self, data):
        contains_kupil = data['text'].str.contains('üü¢–∫—É–ø–∏–ª')
        contains_prodal = data['text'].str.contains('‚≠ïÔ∏è–ø—Ä–æ–¥–∞–ª')

        k_indexes = contains_kupil.where(contains_kupil == True).dropna().index.values.tolist()
        p_indexes = contains_prodal.where(contains_prodal == True).dropna().index.values.tolist()

        # –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã
        return k_indexes + p_indexes

    def make_embeddings(self, data):
        embds = self.model_for_embeddings.encode(data)
        return embds

    def delete_duplicate(self, data):
        data = data[data['unique'] == 1]
        logging.warning('\n Model: duplicates are deleted')
        return data

    def clean_text(self, text):
        text = str(text)
        return str(re.sub("[\(\[].*?[\)\]]", "", text))

    def id2cat(self, i: int) -> str:
        categories = ['–≠–∫–æ–Ω–æ–º–∏–∫–∞', '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–ü–æ–ª–∏—Ç–∏–∫–∞', '–®–æ—É–±–∏–∑', '–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã', '–ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è',
                      '–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ –ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å–Ω–æ–µ', '–†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —é–º–æ—Ä', '–ù–æ–≤–æ—Å—Ç–∏ –∏ –°–ú–ò',
                      '–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è', '–ò—Å–∫—É—Å—Å—Ç–≤–æ', '–°–ø–æ—Ä—Ç', '–¶–∏—Ç–∞—Ç—ã', '–ï–¥–∞ –∏ –∫—É–ª–∏–Ω–∞—Ä–∏—è', '–ë–ª–æ–≥–∏', '–ë–∏–∑–Ω–µ—Å –∏ —Å—Ç–∞—Ä—Ç–∞–ø—ã',
                      '–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥, PR, —Ä–µ–∫–ª–∞–º–∞', '–î–∏–∑–∞–π–Ω', '–ü—Ä–∞–≤–æ',
                      '–ú–æ–¥–∞ –∏ –∫—Ä–∞—Å–æ—Ç–∞', '–ó–¥–æ—Ä–æ–≤—å–µ –∏ –º–µ–¥–∏—Ü–∏–Ω–∞', '–°–æ—Ñ—Ç –∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è', '–í–∏–¥–µ–æ –∏ —Ñ–∏–ª—å–º—ã', '–ú—É–∑—ã–∫–∞', '–ò–≥—Ä—ã',
                      '–†—É–∫–æ–¥–µ–ª–∏–µ', '–î—Ä—É–≥–æ–µ']
        name = categories[i]
        return name
